{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras import regularizers\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import shutil\n",
    "from tensorflow.keras.models import model_from_json\n",
    "!pip install tqdm\n",
    "!pip install h5py\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x):\n",
    "    \n",
    "    return (x - np.mean(x)) / np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \n",
    "    return x[:,:,:int(0.75*x.shape[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/pk2573\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path + \"/Data\"):\n",
    "    os.mkdir(path + \"/Data\")\n",
    "    os.mkdir(path + \"/Data/Data\")\n",
    "    os.mkdir(path + \"/Data/Labels\")\n",
    "    os.mkdir(path + \"/Data/Labels/Tau_23\")\n",
    "    os.mkdir(path + \"/Data/Labels/Tau_12\")\n",
    "    os.mkdir(path + \"/Data/Labels/Tau_13\")\n",
    "else:\n",
    "    shutil.rmtree(path + \"/Data\")\n",
    "    os.mkdir(path + \"/Data\")\n",
    "    os.mkdir(path + \"/Data/Data\")\n",
    "    os.mkdir(path + \"/Data/Labels\")\n",
    "    os.mkdir(path + \"/Data/Labels/Tau_23\")\n",
    "    os.mkdir(path + \"/Data/Labels/Tau_12\")\n",
    "    os.mkdir(path + \"/Data/Labels/Tau_13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_taus = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generae Input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs = [subdir for subdir, dirs, files in os.walk(path + \"/Capstone_Data\")]\n",
    "subdirs = subdirs[1:]\n",
    "subdirs = subdirs[:1] + subdirs[2:]\n",
    "subdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "size = 3\n",
    "for directory in subdirs:\n",
    "    temp = directory.split(\"/\")[-1]\n",
    "    os.chdir(path + \"/Capstone_Data/\" + temp)\n",
    "    print(\"Reading: \" + temp)\n",
    "    \n",
    "    # Load data and check its shape\n",
    "    u = preprocess(scale(loadmat('u_F_xyz_T1.mat')[\"u_F\"]))\n",
    "    v = preprocess(scale(loadmat('v_F_xyz_T1.mat')[\"v_F\"]))\n",
    "    w = preprocess(scale(loadmat('w_F_xyz_T1.mat')[\"w_F\"]))\n",
    "    tau_12 = preprocess(scale(loadmat('tau12_xyz_T1.mat')[\"tau12\"]))\n",
    "    tau_13 = preprocess(scale(loadmat('tau13_xyz_T1.mat')[\"tau13\"]))\n",
    "    tau_23 = preprocess(scale(loadmat('tau23_xyz_T1.mat')[\"tau23\"]))\n",
    "    print(\"Data Shape: \" + str(u.shape))\n",
    "    \n",
    "    x = np.array([u, v, w])\n",
    "    x = np.transpose(x, [1, 2, 3, 0])\n",
    "    x = np.pad(x, ((size,size), (size,size), (size,size), (0,0)), 'constant', constant_values = 0)\n",
    "    del u\n",
    "    del v\n",
    "    del w\n",
    "    gc.collect()\n",
    "    \n",
    "    sample = []\n",
    "    y_tau_23 = []\n",
    "    y_tau_12 = []\n",
    "    y_tau_13 = []\n",
    "    num_files = 10\n",
    "\n",
    "    x_range = np.arange(size, x.shape[0] - size)\n",
    "    np.random.shuffle(x_range)\n",
    "    y_range = np.arange(size, x.shape[1] - size)\n",
    "    np.random.shuffle(y_range)\n",
    "    z_range = np.arange(size, x.shape[2] - size)\n",
    "    np.random.shuffle(z_range)\n",
    "\n",
    "    for i in x_range:\n",
    "        for j in y_range:\n",
    "            for k in z_range:\n",
    "                sample.append(x[i - size: i + size + 1, j - size: j + size + 1, k - size: k + size + 1, :])\n",
    "                y_tau_23.append(tau_23[i - size][j - size][k - size])\n",
    "                y_tau_12.append(tau_12[i - size][j - size][k - size])\n",
    "                y_tau_13.append(tau_13[i - size][j - size][k - size])\n",
    "\n",
    "                if len(sample) == int(((x.shape[0] - size * 2) * (x.shape[1] - size * 2) * (x.shape[2] - size * 2)) / num_files):\n",
    "                    os.chdir(path + \"/Data/Data\")\n",
    "                    np.save(str(counter), np.array(sample))\n",
    "                    os.chdir(path + \"/Data/Labels/Tau_23\")\n",
    "                    np.save(str(counter), np.array(y_tau_23))\n",
    "                    os.chdir(path + \"/Data/Labels/Tau_12\")\n",
    "                    np.save(str(counter), np.array(y_tau_12))\n",
    "                    os.chdir(path + \"/Data/Labels/Tau_13\")\n",
    "                    np.save(str(counter), np.array(y_tau_13))\n",
    "\n",
    "                    del sample\n",
    "                    del y_tau_13\n",
    "                    del y_tau_12\n",
    "                    del y_tau_23\n",
    "                    gc.collect()\n",
    "                    sample = []\n",
    "                    y_tau_23 = []\n",
    "                    y_tau_12 = []\n",
    "                    y_tau_13 = []\n",
    "                    \n",
    "                    counter += 1\n",
    "    del x\n",
    "    del tau_12\n",
    "    del tau_13\n",
    "    del tau_23\n",
    "    gc.collect()\n",
    "    print(\"Saved \" + temp + \" to disk\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    \n",
    "    def __init__(self, activation, initializer, regularizer, input_shape):\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.regularizer = regularizer\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        pass \n",
    "    \n",
    "    def create_model(self):\n",
    "        model = keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape = self.input_shape),\n",
    "            tf.keras.layers.Dense(128, \n",
    "                             activation = self.activation, \n",
    "                             kernel_regularizer = self.regularizer, \n",
    "                             kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(64, \n",
    "                             activation = self.activation, \n",
    "                             kernel_regularizer = self.regularizer, \n",
    "                             kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(32, \n",
    "                             activation = self.activation, \n",
    "                             kernel_regularizer = self.regularizer, \n",
    "                             kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.nn.relu\n",
    "initializer = None\n",
    "regularizer = None\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "val_split = 0.2\n",
    "\n",
    "datasets = [\"Tau_12\", \"Tau_13\", \"Tau_23\"]\n",
    "results = {}\n",
    "i = 1\n",
    "k = 1\n",
    "print(\"Box size used: \" + str((2 * size + 1, 2 * size + 1, 2 * size + 1, 3)))\n",
    "fig = plt.figure(figsize = (20, 6))\n",
    "for name in datasets:\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    print(\"\\nPredicting \" + name)\n",
    "    input_shape = (2 * size + 1, 2 * size + 1, 2 * size + 1, 3)\n",
    "    neural_net = DNN(activation, initializer, regularizer, input_shape)\n",
    "    model = neural_net.create_model()\n",
    "    model.compile(optimizer = tf.train.AdamOptimizer(), loss = \"mse\")\n",
    "    correlation = []\n",
    "    R2 = []\n",
    "    for i in range(epochs):\n",
    "        temp_1 = []\n",
    "        temp_2 = []        \n",
    "        files_range = np.arange(1, num_files * 4 + 1)\n",
    "        np.random.shuffle(files_range)\n",
    "        for j in files_range:\n",
    "            # Load Datasets\n",
    "            x = np.load(path + \"/Data/Data/\" + str(j) + \".npy\")\n",
    "            y = np.load(path + \"/Data/Labels/\" + name + \"/\" + str(j) + \".npy\")\n",
    "            \n",
    "            # Create Train and Test sets\n",
    "            mask =  np.random.rand(x.shape[0]) < 0.80\n",
    "            x_train = x[mask,:,:,:,:]\n",
    "            y_train = y[mask]\n",
    "            x_test = x[~mask,:,:,:,:]\n",
    "            y_test = y[~mask]\n",
    "            del y\n",
    "            del mask\n",
    "            gc.collect()\n",
    "\n",
    "            model.fit(x_train, y_train, epochs = 1, validation_split = 0.2, batch_size = batch_size, verbose = 0)\n",
    "            del x_train\n",
    "            del y_train\n",
    "            gc.collect()\n",
    "            \n",
    "            # Get Test corr and R2 and append to list\n",
    "            y_pred = model.predict(x_test).flatten()\n",
    "            gc.collect()\n",
    "            y_true = y_test.flatten()\n",
    "            temp_1.append(r2_score(y_true, y_pred))\n",
    "            temp_2.append(np.corrcoef(y_pred, y_true)[0, 1])\n",
    "            del x_test\n",
    "            del y_test\n",
    "            del x\n",
    "            gc.collect()\n",
    "            i += 1\n",
    "        \n",
    "        # Get average Test corr and R2 for the epoch\n",
    "        correlation.append(np.mean(temp_2))\n",
    "        R2.append(np.mean(temp_1))\n",
    "    \n",
    "    # Plot R2 and corr evolution\n",
    "    fig.add_subplot(1, len(datasets), k)\n",
    "    plt.plot(np.arange(1, epochs + 1), np.array(R2), label = 'R2')\n",
    "    plt.plot(np.arange(1, epochs + 1), np.array(correlation), label = 'Correlation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    print(\"Final Dataset Correlation: %.4f\" % np.corrcoef(y_pred, y_true)[0, 1])\n",
    "    del y_pred\n",
    "    del y_true\n",
    "    gc.collect()\n",
    "    \n",
    "    k += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
